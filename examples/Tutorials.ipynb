{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "29a9fcdd",
            "metadata": {},
            "source": [
                "![Degirum banner](https://raw.githubusercontent.com/DeGirum/PySDKExamples/main/images/degirum_banner.png)\n",
                "# Face Recognition & Tracking Tutorials\n",
                "This notebook has two tutorials:\n",
                "1. How to perform face recognition tasks step by step\n",
                "2. How to perform real-time face tracking"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f6dd8f8c",
            "metadata": {},
            "source": [
                "Prerequisites (uncomment to execute):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "20408aab",
            "metadata": {},
            "outputs": [],
            "source": [
                "# install degirum-face package\n",
                "# %pip install degirum_face\n",
                "\n",
                "# create cloud API access token\n",
                "# !degirum token create"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "aa760bd4",
            "metadata": {},
            "source": [
                "### Decide where your hardware is located\n",
                "1. @cloud: To run on hardware hosted in DeGirum AI Hub\n",
                "1. @local: To run on you current host\n",
                "1. <host_ip>: To run on an AI server either on the same machine ('localhost') or in local LAN\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1c39bb31",
            "metadata": {},
            "outputs": [],
            "source": [
                "inference_host_address = \"@cloud\"  # hardware location"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a4405222",
            "metadata": {},
            "source": [
                "### Explore available hardware options"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dd5eb794",
            "metadata": {},
            "outputs": [],
            "source": [
                "import degirum_face\n",
                "\n",
                "# What's in the registry?\n",
                "registry_hw = degirum_face.model_registry.get_hardware()\n",
                "print(f\"Registry has {len(registry_hw)} hardware types:\")\n",
                "print(registry_hw)\n",
                "\n",
                "# What's available on inference_host_address?\n",
                "available_hw = degirum_face.get_system_hw(inference_host_address)\n",
                "print(f\"\\n{inference_host_address} has {len(available_hw)} hardware types:\")\n",
                "print(available_hw)\n",
                "\n",
                "# What can I actually use? (intersection)\n",
                "compatible_hw = degirum_face.get_compatible_hw(inference_host_address)\n",
                "print(f\"\\nYou can use {len(compatible_hw)} hardware types:\")\n",
                "print(compatible_hw)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "34cfe62e",
            "metadata": {},
            "source": [
                "### Decide which hardware to use\n",
                "Choose one from `compatible_hw` printed just above"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f493c7e4",
            "metadata": {},
            "outputs": [],
            "source": [
                "hardware_to_use = \"N2X/ORCA1\"  # hardware to use for model inference. Choose one from compatible_hw above"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "69b55b4b",
            "metadata": {},
            "source": [
                "### Face Recognition Tutorial\n",
                "\n",
                "Define all necessary objects:\n",
                "- face embeddings database\n",
                "- face recognition configuration\n",
                "- create face recognition object"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c997fa7f",
            "metadata": {},
            "outputs": [],
            "source": [
                "import degirum_face, degirum_tools, numpy as np\n",
                "\n",
                "# define face recognition configuration\n",
                "face_recognition_config = degirum_face.FaceRecognizerConfig(\n",
                "    face_detection_model_spec=degirum_face.get_face_detection_model_spec(\n",
                "        hardware_to_use, inference_host_address=inference_host_address\n",
                "    ),\n",
                "    face_embedding_model_spec=degirum_face.get_face_embedding_model_spec(\n",
                "        hardware_to_use, inference_host_address=inference_host_address\n",
                "    ),\n",
                "    db_path=\"temp/tutorial_db.lance\",\n",
                ")\n",
                "\n",
                "# create FaceRecognizer instance: this is the main object to use for face recognition\n",
                "face_recognizer = degirum_face.FaceRecognizer(face_recognition_config)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c6cff169",
            "metadata": {},
            "source": [
                "Define helper function to display images and videos in Jupyter notebooks\n",
                "(this is just a set of service functions for convenience)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8313277f",
            "metadata": {},
            "outputs": [],
            "source": [
                "from IPython.display import Image, Video, display\n",
                "import cv2, tempfile\n",
                "\n",
                "\n",
                "def image_display(img):\n",
                "    display(Image(data=cv2.imencode(\".png\", img)[1].tobytes()))\n",
                "\n",
                "\n",
                "def video_display(video_bytes):\n",
                "    with tempfile.TemporaryDirectory() as tmpdir:\n",
                "        video_path = f\"{tmpdir}/video.mp4\"\n",
                "        with open(video_path, \"wb\") as f:\n",
                "            f.write(video_bytes)\n",
                "\n",
                "        display(Video(filename=video_path, width=640, height=480, embed=True))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e4946440",
            "metadata": {},
            "source": [
                "Enroll persons in the database by analyzing their face images to extract and store face embeddings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4ebd5363",
            "metadata": {},
            "outputs": [],
            "source": [
                "# clear all tables to start fresh\n",
                "face_recognizer.db.clear_all_tables()\n",
                "\n",
                "# enroll Alice and Bob\n",
                "enrolled = face_recognizer.enroll_batch(\n",
                "    (\"assets/Alice-1.png\", \"assets/Bob-1.png\"), (\"Alice\", \"Bob\")\n",
                ")\n",
                "\n",
                "# print enrolled face information\n",
                "print(\"Frame Name    DB ID\")\n",
                "for face in enrolled:\n",
                "    print(f\"{face.frame_id:5} {face.attributes:7} {face.db_id}\")\n",
                "\n",
                "# show number of enrolled embeddings in database\n",
                "print(\"\\n\", face_recognizer.db.count_embeddings())\n",
                "\n",
                "enrolled_embeddings = [face.embeddings[0] for face in enrolled]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "43adb9b9",
            "metadata": {},
            "source": [
                "Recognize enrolled persons using their other images"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "018f87bf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# loop over other images\n",
                "recognized_embeddings = []\n",
                "for result in face_recognizer.predict_batch(\n",
                "    (\n",
                "        \"assets/Alice-2.png\",\n",
                "        \"assets/Alice-3.png\",\n",
                "        \"assets/Bob-2.png\",\n",
                "        \"assets/Bob-3.png\",\n",
                "        \"assets/Alice&Bob.png\",\n",
                "    )\n",
                "):\n",
                "    # print recognition results\n",
                "    print(f\"\\nImage '{result.info}'---------------\")\n",
                "    for n, face in enumerate(result.faces):\n",
                "        print(f\"Face #{n}:\\n{face}\")\n",
                "        recognized_embeddings.append(face.embeddings[0])\n",
                "\n",
                "    # display image overlay\n",
                "    image_display(result.image_overlay)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c86361a3",
            "metadata": {},
            "source": [
                "Compute pairwise cosine similarities over face embeddings (just for demo purposes)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1ecc92bb",
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "\n",
                "# Compute cosine similarity between every pair (recognized_embeddings, enrolled_embeddings)\n",
                "sim_matrix = cosine_similarity(np.array(recognized_embeddings), np.array(enrolled_embeddings))\n",
                "print(\"Cosine similarity matrix\\n [Alice      Bob       ]\")\n",
                "print(sim_matrix)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4ca922ce",
            "metadata": {},
            "source": [
                "### Face Tracking Tutorial\n",
                "\n",
                "Define face tracking configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d0bc3f8f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# define configuration for face filtering\n",
                "face_filters = degirum_face.FaceFilterConfig(\n",
                "    enable_small_face_filter=True,\n",
                "    min_face_size=30,\n",
                "    enable_zone_filter=True,\n",
                "    zone=[[100, 10], [960, 10], [960, 700], [100, 700]],\n",
                "    enable_frontal_filter=True,\n",
                "    enable_shift_filter=True,\n",
                "    enable_reid_expiration_filter=True,\n",
                "    reid_expiration_frames=12,\n",
                ")\n",
                "\n",
                "# define configuration for clip storage (use local directory for tutorial, but can be any S3-compatible storage)\n",
                "clip_storage_config = degirum_tools.ObjectStorageConfig(\n",
                "    endpoint=\"./temp\", access_key=\"\", secret_key=\"\", bucket=\"tutorial_videos\"\n",
                ")\n",
                "\n",
                "# define face tracking configuration\n",
                "face_tracking_config = degirum_face.FaceTrackerConfig(\n",
                "    video_source=\"assets/WalkingPeople.mp4\",\n",
                "    face_detection_model_spec=degirum_face.get_face_detection_model_spec(\n",
                "        hardware_to_use, inference_host_address=inference_host_address\n",
                "    ),\n",
                "    face_embedding_model_spec=degirum_face.get_face_embedding_model_spec(\n",
                "        hardware_to_use, inference_host_address=inference_host_address\n",
                "    ),\n",
                "    db_path=\"temp/tutorial_db.lance\",\n",
                "    face_filters=face_filters,\n",
                "    clip_storage_config=clip_storage_config,\n",
                "    clip_duration=300,\n",
                "    alert_mode=degirum_face.AlertMode.ON_UNKNOWNS,\n",
                "    credence_count=4,\n",
                ")\n",
                "\n",
                "# create clip manager instance\n",
                "clip_manager = degirum_face.FaceClipManager(clip_storage_config)\n",
                "\n",
                "# create FaceTracker instance\n",
                "face_tracker = degirum_face.FaceTracker(face_tracking_config)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e859e2cd",
            "metadata": {},
            "source": [
                "Run pipeline on database. We will collect video clips of unknown persons."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6fa1a515",
            "metadata": {},
            "outputs": [],
            "source": [
                "# clear all saved clips first\n",
                "clip_manager.remove_all_clips()\n",
                "\n",
                "# then run the face tracking pipeline\n",
                "composition, _ = face_tracker.start_face_tracking_pipeline()\n",
                "composition.wait()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c1b9c8a6",
            "metadata": {},
            "source": [
                "Analyze collected video clips"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8c63d005",
            "metadata": {},
            "outputs": [],
            "source": [
                "# list all collected clips\n",
                "all_clips = clip_manager.list_clips()\n",
                "print(f\"Clips saved: {len(all_clips)}\")\n",
                "\n",
                "# show them by downloading from storage:\n",
                "for clip in all_clips:\n",
                "    video_display(clip_manager.download_file(clip + \".mp4\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "77a7b54c",
            "metadata": {},
            "source": [
                "Annotate collected video clips"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6a25514b",
            "metadata": {},
            "outputs": [],
            "source": [
                "for clip in all_clips:\n",
                "    # annotate clip: it will return face map object indexed by object track IDs and containing face embeddings\n",
                "    face_map = face_tracker.find_faces_in_clip(clip)\n",
                "\n",
                "    # show annotated clip\n",
                "    video_display(\n",
                "        clip_manager.download_file(clip + clip_manager.annotated_video_suffix)\n",
                "    )\n",
                "\n",
                "    break  # annotate only first clip"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ee73bf9c",
            "metadata": {},
            "source": [
                "Add (enroll) all persons detected on the video clip to the database"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "35ab65f2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# You can see on a video above that object #1 is Bob and object #2 is Alice\n",
                "\n",
                "# assign names to face objects\n",
                "face_map[1].attributes = \"Bob\"\n",
                "face_map[2].attributes = \"Alice\"\n",
                "\n",
                "# enroll faces into the database\n",
                "face_tracker.enroll(face_map.values())\n",
                "\n",
                "# show number of enrolled embeddings in database\n",
                "print(face_tracker.db.count_embeddings())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ae915429",
            "metadata": {},
            "source": [
                "Run pipeline one more time: now should be no alerts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e92ec88d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# clear all saved clips first\n",
                "clip_manager.remove_all_clips()\n",
                "\n",
                "# start face tracking pipeline\n",
                "composition, _ = face_tracker.start_face_tracking_pipeline()\n",
                "composition.wait()\n",
                "\n",
                "clips = clip_manager.list_clips()\n",
                "print(f\"Now we have: {len(clips)} clip(s)\")\n",
                "if not clips:\n",
                "    print(\"No alerts detected!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
